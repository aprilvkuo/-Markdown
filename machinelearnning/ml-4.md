---
title: ml-4---神经网络
date: 2017-01-06 16:18:56
tags: [Maching Learning]
categories: [机器学习]
---

# 感知机
感知机的计算输出:  
$ o(x_1,...,x_n) = 1 if __w_0__ +w_1x_1+...+w_nx_n>0 else 0 $
$ w_i $为权值, $-w_0 $ 为阈值
感知机函数可以用 sgn函数 以及向量的形式表示.
感知机的表征能力:可以表示所有的原子布尔函数.

# 感知机训练法则
修改与输入$x_i$对应的权值$w_i$:
$ w_i \leftarrow  w_i + \Delta w_i$
其中 $ \Delta w_i = \eta (t-o) x_i$
t(true)为样例的真实值.o(output)为感知机的输出. $ \eta $为学习速率.

证明:当o和t相同时,w不会有变化.
    当o为1,t为0时.t-o=-1.如果x > 0,则$ \Delta w_i $ < 0,w变小,则wx变小,o变小,往0的方向靠拢,同理可推其他情况.

# 梯度下降 与 delta法则

delta法则的关键思想是使用梯度下降来搜索可能的权向量的假设空间,找到最佳拟合的训练样例的权向量.
对于包含不用类型的 __连续__ 参数化假设的假设空间,梯度下降是遍历这样假设空间的所有算法的基础.

delata 法则可以理解为一个 __没有阈值__ 的感知机,即一个 __线性单元__ :
$ o( \vec{x})=\vec{w} \vec{x}$

衡量假设相对于训练样例的训练误差,以下是一个简单的的标准:
$E( \vec{w})={1 \over 2 }\sum{(t_d - o_d)^2}$

# 梯度下降法则的推导

E相对于向量 $ \vec{w}$的导数称为梯度,记为 $ \nabla E(\vec{w}) $,它定义的是使E最陡峭上升的方向.反方向为下降的方向.
所以 $\vec{w} \leftarrow \vec{w} + \Delta \vec{w}$
其中: $ \Delta \vec{w} = -\eta {\partial E \over \partial w_i}$
带入E的公式可以求得
$ { \partial E \over \partial w_i} = \sum (t_d - o_d)(- x_id) $ 即D中的每一个样例d,x为它的第i个分量.所有样例在i上面的操作之和.
所以有:
$ \Delta \vec{w} = -\eta {\partial E \over \partial w_i}  =  -\eta \sum (t_d - o_d)(-x_id)$

> 梯度下降算法: GRADIENT-DESCENT(training_examples, η )
training_examples 中每一训练样例形式为序偶<x,t>,其中 x 式输入向量,t 是目标输出值, η 是学习速率(例如 0.05)
  •初始化每个 wi 为某个小的随机值
  •遇到终止条件之前,做以下操作:
    + 初始化每个Δwi 为 0
    + 对于训练样例 training_examples 中的每个<x,t>,做:
      •把实例 x 输入到此单元,计算输出 o
      •对于线性单元的每个权 wi,做
          __Δwi←Δwi+ η (t-o)xi__
    + 对于线性单元的每个权 wi,做
          wi←wi+Δwi

它的应用满足下面的条件:
1. 假设空间含有 __连续__ 参数化的假设
2. 误差对于这些假设参数 __可微__
遇到的问题:
1. 收敛过程速度特别慢
2. 如果有多个局限最小值,不能保证找到全局最小值

# 增量梯度下降
一个梯度下降的变体缓解上面的难题.上面的梯度下降训练法则在对D中的所有训练样例 __求和__ 后计算权值更新.
随机梯度下降的思想是根据每个单独样例的误差增量计算权值更新.
权值更新更新公式变成:
$ \Delta \vec{w}  =  -\eta (t - o)(-x_i)$
随机梯度下降可以看成每个单独的训练样例d定义不同的误差函数 $ E_d(\vec{w}) = { 1 \over 2 } (t_d - o_d)^2 $

# 标准梯度下降与增量随机下降的区别
1. 标准是在权值更新前对所有样例汇总误差, 随机梯度下降的权值更新是通过每一个训练样例实现的.
2. 标准中,每一个的更新需要对多个样例求和需要很多运算,另一方面,标准对于每一次权值更新使用比梯度下降步长大
3. 如果有多个局部极小值,随机梯度下降可能避免陷入局部极小值.因为他不是使用真正的梯度.

增量原则(delta rule)又叫做LMS法则

# 小结
感知机训练法则根据阈值化的感知机输出的误差更新权值.
增量原则根据输入的非阈值线性组合的误差来更新权.
两者的差异主要表现在不同的收敛特性上,感知机训练法则经过有限次的迭代收敛到一个理想分类训练数据的假设,但是条件是 __训练样例线性可分__.
增量法则渐进收敛到最小误差假设,可能需要很长时间, __但是无论训练样例是否线性可分都会收敛__ .

# 多层网络和反向传播算法
  多层网络可以表示高度非线性的决策面
## 可微阈值单元
  已经推导出来梯度下降学习法则,然而多个线性单元的链接仍然产生线性函数.
  感知器单元是一种选择,但他的不连续阈值使他不可微,所以不适合梯度下降算法.
  我们需要一种单元,它的输出是输入的 __非线性函数__  ,并且输出是输入的可微函数.
  这种但是是sigmoid单元,很类似于感知器的单元,但他基于一个平滑的可微阈值函数.
  sigmoid函数又叫做logistic函数,输出范围为0到1,随输入单调递增,sigmoid函数有一个 __有用的特征__, __它的导数__ 很容易用用它的输出表示 $ {d\sigma(y) \over x} =   \sigma (y) (1- \sigma(y))$

## 反向传播算法
  利用梯度下降方法试图最小化网络输出值和目标值之间的误差平方.现在要考虑多个输出单元的网络,不和前面一样只考虑单个单元,所以需要重新定义误差E:
  $ E( \vec{w})={1 \over 2 }\sum{(t_d - o_d)^2} $``
