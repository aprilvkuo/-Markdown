---
title: 机器学习课---3----一般到特殊序列
date: 2017-01-03 16:07:41
tags: [Maching Learning]
categories: [机器学习]
---

# 定义
  概念学习:从特殊样例中得到一般概念.指从有关某个布尔函数的输入输出训练样例中推断出 __布尔函数__ .

# 术语
  对于一套训练样例X,每个样例为X的一个实例x以及它的目标概念值c(x),对于c(x) = 1 的实例称为 __正例__. 反之为 __负例__.
  H 表示所有可能的假设的集合,H中的每个假设h表示X上定义的布尔函数.即 h: X->{0 , 1}.
  __机器学习的目标__: 寻找一个假设h,使对于X中的所有x,h(x) = c(x).
  __归纳学习假设__: 任一假设如果在足够大的训练样例集中能很好地逼近目标函数,它也能在未见的实例中好好地逼近目标函数
  __偏序关系__ :集合 P上的关系 R 称为 P上的偏序关系，当且仅当 R是自反的、反对称的和传递的。用“≤”表示偏序关系

# FIND-S 寻找最大特殊值算法
  1. 将 h 初始化为 H 中最特殊假设
  2. 对每个正例 x
  - 如果 h(x)≠c(x),则用覆盖 x 的 h 的极小普化式替代 h(下一个更一般约束代替)
  3. 输出假设 h

  > 缺点
  1. 无法确定是否收敛到正确的目标概念
  2. 当训练集不一样时候,出现错误或者噪声时,会破坏FIND-S算法.
  3. 为什么要选择最特殊的假设
  4. 可能会有多个极大特殊假设.

# 变型空间 Version Spaces 与 候选消除算法
  __一致__ :假设h与训练样例集合D,当且仅当D中的每一个样例<x, c(x)>有h(x) = c(x)
  __变型空间 $ V S_{H,D} $__ : 对于假设空间H和训练样例集合D,H中与训练样例D一致的所有假设构成的子集.

## 列表后消除算法
  1. 变型空间 VersionSpace ← 包含 H 所有假设的列表
  2. 对每个训练样例 <x,c(x)>
    从变型空间中移除所有 h(x)≠c(x)的假设 h
  3. 输出 VersionSpace 中的假设列表
  现将变型空间初始为所有的包含H的假设.

## 表示方法
  G:极大一般成员    S:极大特殊成员

## 候选消除算法
  >其实类似于上界下界.特殊集合与一般集合,正例应该处于它们之间.(整个过程需要移除两头不一致的假设)
  如果正例不在它们之间,要将特殊集合s一般化以满足.而且如果一般集合中有分支k不满足也应该删除.
  如果反例在它们之间,则应该将一般集合g特殊化使其不满足

  将 G 集合初始化为 H 中极大一般假设
  将 S 集合初始化为 H 中极大特殊假设
  对每个训练样例 d,进行以下操作:
  ·如果 d 是一正例
    ·从 G 中移去所有与 d 不一致的假设
    ·对 S 中每个与 d 不一致的假设 s
      ·从 S 中移去 s
    · 把 s 的所有的极小泛化式 h 加入到 S 中,其中 h 满足
      ·h 与 d 一致,而且 G 的某个成员比 h 更一般
    ·从 S 中移去所有这样的假设:它比 S 中另一假设更一般
  ·如果 d 是一反例
    ·从 S 中移去所有与 d 不一致的假设
    ·对 G 中每个与 d 不一致的假设 g
    ·从 G 中移去 g
    ·把 g 的所有的极小特殊化式 h 加入到 S 中,其中h 满足
    ·h 与 d 一致,而且 S 的某个成员比 h 更特殊
    ·从 G 中移去所有这样的假设:它比 G 中另一假设更特殊

## 候选消除算法是否会收敛到正确的假设
  当S和G收敛到一个空的变型空间,可以得知训练数据出错.
## 需要确定怎样的训练样例
  每次新的样例使得变型空间大小减少一半.
## 如何利用不完全的学习概念
  形成一般到特殊的模型图后,对于待预测的实例,

# 归纳偏置
由于归纳学习需要某种形式的预先假定,称为归纳偏置.
如果学习器是无偏的，那么它根本上无法对未见实例进行分类，这一点很重要.
如果训练样例没有错误，初始假设空间包含概念目标时，如果提供足够多的训 练样例，候选消除算法可以收敛到目标概念。前面提到，断言假设的形式为属性的合取，事实上是为了缩小需要搜索的假设空间的范围。这样做的一个后果是，有可 能目标概念不在这样一个初始的假设空间中。

如果想要保证假设空间中包含目标概念，一个明显的方法是扩大假设空间，使每个可能的 假设都被包含在内。再次以EnjoySport为例子，其中将假设空间限制为只包含属性值的合取。由于这一限制，假设空间不能够表示最简单的析取形式的目 标如“Sky = Sunny 或 Sky = Cloudy”。

所以问题在于，我们使学习器偏向于只考虑合取的假设。

如果使用无偏的假设空间，概念学习算法将无法从训练样例从泛化，要想得到 单个目标概念，必须提供X中所有的实例作为训练样例。我们根本不能从这样一个学习器中，得到对未知实例的分类。
无偏学习的无用性


好吧，居然这种偏向可能使得假设空间漏掉了目标概念，那我们就提供一个表达能力更强的空间，它能表达所有的可教授概念。换言之，它能够表达实例集X的所有可能的子集。一般我们把集合X的所有子集的集合称为X的幂集（power set）。

假设AirTemp、Humidity、Wind、Water、Forcast都只 有两种可能取值，Sky有三种可能取值，那么实例空间X包含了3×2×2×2×2×2=96种不同的实例。根据集合的知识，在这一实例集合X的幂集的大小 是2^|X|，其中|X|是X的元素数目。因此在这一实例空间上可以定义2^96，或大约10^28个不同的目标概念，我们称包含了2^|X|个假设的这 样一个假设空间是一个无偏的假设空间。先前我们将假设空间限制为只包含属性值的合取，那么只能表示1+4×3×3×3×3×3=973个假设。哈，看来我们先前的空间实在是一个偏置很大的假设空间。

从感觉上讲，无偏的假设空间虽然一定包含了目标概念，但是它包含的假设的数量太大， 搜索这样一个空间必然会很费时。然而，你马上会发现，这里还存在一个根本的问题：如果使用无偏的假设空间，概念学习算法将无法从训练样例从泛化，要想得到 单个目标概念，必须提供X中所有的实例作为训练样例。我们根本不能从这样一个学习器中，得到对未知实例的分类。

现在来看看为什么这么说。假定我们给学习器提供了3个正例( x1, x2, x3)以及反例(x4, x5)。这时，变型空间的S边界包含的假设正好是三个正例的析取：

S : { (x1 ∨ x2 ∨ x3) }

因为这是能覆盖3个正例的最特殊假设。相似地，G边界将由那些刚好能排除掉的那些假设组成。

G : {否定符号(x4 ∨ x5) }

问题来了，在这一非常具有表达力的假设表示方法中，S边界总是所有正例的析取式，G边界总是所有反例的析取的否定式。这样能够由S和G无歧义分类的，只有已见到的训练样例本身。要想获得单个目标概念，就必须提供提供X中所有的实例作为训练样例。

好吧，为了避免这个问题，我们只使用不完全学习得到的变型空间，像前面使用成员投票 的方式对未见实例进行分类。遗憾的是，你马上会发现投票对于那些未见过的实例不起作用，为什么？未见实例会被变形空间中刚好半数的假设划分为正例，而被另 一半划分为反例，原因如下，若H是X的幂集，而x是某个未出现的实例，则对于变型空间中一覆盖x的假设h。必然存在另一假设h’，它与h几乎相等，只不过 对x的分类不同。而且，如果h在变型空间中，那么h’也在，因为它对于已往训练样例的划分与h完全一样。

以上讨论说明了归纳推理的一个基本属性：学习器如果不对目标概念的形式做预先的假定，它从根本上无法对未见实例进行分类。这便是无偏学习的无用性。

我们原来的EnjoySport任务中，候选消除算法能够从训练样例中泛化，惟一的原因就是它是有偏的，隐含假定了目标概念可以由属性值的合取来表示。

由于归纳学习需要某种形式的预先假定，或称为归纳偏置（inductive bias），我们可以用归纳偏置来描述不同学习方法的特征。

归纳偏置还有一个更术语化的定义，这里就不提了。简单的讲，归纳偏置就是一个附加的 前提集合B，以后还会提到，这个前提集合B有两种情况，第一种是对假设空间进行限定，就像候选消除算法那样；第二种是假设空间是完整的假设空间，但是进行 不彻底的搜索，很多贪心算法都是这样的，如以后会提到的决策树算法。前一种归纳偏置叫做限定偏置，后一种叫做优选偏置。

在研究其他的归纳推理方法时，有必要牢记这种归纳偏置的存在及其强度。一种算法如果有偏性越强，那它的归纳能力越强，可以分类更多的未见实例。当然，分类的正确性也依赖于归纳偏置的正确性。
