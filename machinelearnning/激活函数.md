# 激活函数

> 可微性。
>
> 单调性：单层网络保证是凸函数。
>
> f(x)约等于x：
>
> 输出值有范围： 这样基于梯度的优化方法会更加稳定。

 **the role of activation functions is make neural networks non-linear**

## Sigmoid

特点： 压缩到0到1之间。

缺点：

1. 当输入特别大或者特别小时候，梯度接近于0.
2. output不是0均值。因为输入值一直是正值，所以bp过程中梯度可能恒为正或负。但是batch能够缓解部分问题。

## Tanh

tanh 跟sigmoid还是很像的，实际上，tanh 是sigmoid的变形

其实  tanh(x)=2σ(2x)−1。

由于是0均值的，所以比sigmoid好。

## Relu

优点：

1. 发现使用 ReLU 得到的SGD的收敛速度会比 sigmoid/tanh 快很多(看右图)。有人说这是因为它是linear，而且 non-saturating 
2. 相比于 sigmoid/tanh，ReLU 只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。

缺点： 

很脆弱。

一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了。

## Leakly-ReLu

就是用来解决这个 **"dying ReLU"** 的问题的。与 ReLU 不同的是：
$$
f(x)= \alpha x (x<0);
f(x) = x(x>=0)
$$
关于Leaky ReLU 的效果，众说纷纭，没有清晰的定论。有些人做了实验发现 Leaky ReLU 表现的很好；有些实验则证明并不是这样。



![比较图](https://pic2.zhimg.com/a7f872a80b7223ca5bb7690497ab2239_b.png)