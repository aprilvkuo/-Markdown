---
title: knn-bayes
date: 2017-03-15 20:48:14
tags: [Maching Learning]
---

# k近邻法

## 算法的主要思想:输入一个新的实例,在训练集中找到与该实例最邻近的k个实例,用投票的方法给这个实例一个最多数类(多数表决)

根据度量,在训练集中找到与x最邻近的k个点.

## 距离度量
kNN的特征空间一般是n维向量空间 $R^n$ , 使用欧式距离,但是也可以是其他距离,如更一般的 $L_p$ 距离
$L_p$距离的定义为:
$L_p(x_i,x_j) = (\sum {|x_i^l-x_j^l|^p})^{}(1/p)}$
当p为2时候,为欧式距离
p为1时候,为曼哈顿距离
p为正无穷时候,  $L=max|x_i-x_j|$,是各个坐标距离的最大值.

## k值的选择
k值的选择很重要
__近似误差(approximation error)__: 度量与最优误差之间的相近程度                                                                                                                                                                                                                                                                                 
__估计误差(estimation error)__: 度量预测结果与最优结果的相近程度
当k值减少时,只有与输入点进的点才会起预测效果,近似误差减少,但是对于点特别敏感,估计误差增大.当k值减少时,模型变得复杂,更容易拟合.

##分类决策
多数表决,经验风险最小化=>误分率最小

## kd树
kd树可以有效的减少knn的扫描次数.
见统计学习方法
# 朴素贝叶斯法
朴素贝叶斯 != 贝叶斯估计(贝叶斯估计中加入了平滑)
朴素贝叶斯法学习到生成数据的机制,属于生成模型.条件独立假设等于是说用于分类的特征在类确定的条件下都是独立的.
P(Y|X) = P(XY)/P(X)
然后由于P(X)是不变的,只需要最大化分母.
最大化上面的后验概率,将类分到后验概率最大的类,等价于期望风险最小化.可以用0,1损失函数来表示.
## 算法流程:
1. 先计算先验概率和条件概率
2. 对于实例X: 计算P(Y)P(X|Y)
3. 确定实例x的类

## 贝叶斯估计
因为要加入的概率可能为0,所以加入了平滑,最普遍的是拉普拉斯平滑(+1平滑)
