---
title: 各种机器学习中的激活函数,熵和cnn
date: 2017-01-31 14:13:33
tags: [Maching Learning]
categories: [机器学习]
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# sigmoid 函数
逻辑回归:
$$f(x) = {L\over 1+e^{-k(x-x_0)}}$$
逻辑回归的取值范围在0-1之间,且单调地址.当x为0时候,f(x)为1/2.

# softmax 函数

同样，我们贴一下wiki百科对softmax函数的定义：

softmax is a generalization of logistic function that “squashes”(maps) a K-dimensional vector z of arbitrary real values to a K-dimensional vector σ(z) of real values in the range (0, 1) that add up to 1.

可以看出softmax 与 logistic 存在联系,前者是后者的泛化,设输入的向量为x且为k维.则softmax则输出的是k维的向量y,且 $y_i > 0  and \sum y = 1$.

逻辑回归其实是输出
softmax函数形式如下：
$$\sigma (z)_j ={e^{z_j}\over {\sum_{i = 1}^K e^{z_k}}}$$

softmax得到一个k维的向量,每一个维度分别为到每一个类别的概率.
 softmax 回归模型中存在冗余的参数。更正式一点来说， Softmax 模型被过度参数化了。对于任意一个用于拟合数据的假设函数，可以求出多组参数值，这些参数得到的是完全相同的假设函数 $\textstyle h_\theta$。

进一步而言，如果参数 $\textstyle (\theta_1, \theta_2,\ldots, \theta_k)$ 是代价函数 $\textstyle J(\theta)$ 的极小值点，那么 $\textstyle (\theta_1 - \psi, \theta_2 - \psi,\ldots,
\theta_k - \psi)$ 同样也是它的极小值点，其中 $\textstyle \psi$ 可以为任意向量。因此使 $\textstyle J(\theta)$ 最小化的解不是唯一的。（有趣的是，由于 $\textstyle J(\theta)$ 仍然是一个凸函数，因此梯度下降时不会遇到局部最优解的问题。但是 Hessian 矩阵是奇异的/不可逆的，这会直接导致采用牛顿法优化就遇到数值计算的问题）

http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92
 Softmax的理解与应用[http://blog.csdn.net/supercally/article/details/54234115]
 Softmax与交叉熵的应用,导数为正确维度减一.


# 交叉熵函数
http://blog.csdn.net/u012162613/article/details/44239919
交叉熵代价函数为:
$$ C= - {1 \over n} \sum [ylna +(1-y)ln(1-a)]$$
其中y为期望的输出,a为神经元实际输出.
与方差代价函数一样，交叉熵代价函数同样有两个性质：

非负性。（所以我们的目标就是最小化代价函数）
当真实输出a与期望输出y接近的时候，代价函数接近于0.(比如y=0，a～0；y=1，a~1时，代价函数都接近0)。
另外，它可以克服方差代价函数更新权重过慢的问题。从导数可以看出.

- 当我们用sigmoid函数作为神经元的激活函数时，最好使用交叉熵代价函数来替代方差代价函数，以避免训练过程太慢。
- 不过，你也许会问，为什么是交叉熵函数？导数中不带σ′(z)项的函数有无数种，怎么就想到用交叉熵函数？这自然是有来头的，更深入的讨论就不写了，少年请自行了解。
- 另外，交叉熵函数的形式是−[ylna+(1−y)ln(1−a)]而不是 −[alny+(1−a)ln(1−y)]，为什么？因为当期望输出的y=0时，lny没有意义；当期望y=1时，ln(1-y)没有意义。而因为a是sigmoid函数的实际输出，永远不会等于0或1，只会无限接近于0或者1，因此不存在这个问题

# 信息熵,交叉熵,相对熵
1）信息熵：编码方案完美时，最短平均编码长度的是多少。
2）交叉熵：编码方案不一定完美时（由于对概率分布的估计不一定正确），平均编码长度的是多少。
平均编码长度 = 最短平均编码长度 + 一个增量
3）相对熵：编码方案不一定完美时，平均编码长度相对于最小值的增加值。（即上面那个增量）

交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。


1、熵的本质是香农信息量 log(1/p) 的期望；
$$ H(p) = E[ log(1/p) ] = ∑ p_i *log(1/p_i) $$，是一个期望的计算，也是记录随机事件结果的平均编码长度；
为什么信息量 是 $log(1/p)$ 呢？
因为：一个事件结果的出现概率越低，对其编码的bit长度就越长。
以期在整个随机事件的无数次重复试验中，用最少的 bit 去记录整个实验历史。
即无法压缩的表达，代表了真正的信息量。

2、熵的本质的另一种解释：最短平均编码长度；
【本质含义：编码方案完美时，最短平均编码长度的是多少】

3、交叉熵，则可以这样理解：使用了“估算”的编码后，得到的平均编码长度（可能不是最短的）
p是真实概率分布，q是你以为的概率分布（可能不一致）；
你以 q 去编码，编码方案 log(1/q_i)可能不是最优的；
于是，平均编码长度 = $\sum p_i *log(1/q_i)$，就是交叉熵；
只有在估算的分布 q 完全正确时，平均编码长度才是最短的，交叉熵 = 熵


一、交叉熵
1、定义
【本质含义：编码方案不一定完美时，平均编码长度的是多少】
连续函数：
$H(p,q) =  E_p[-log_q] = H(p) + D_{KL}(p||q)$
两项中 H(p)是 p的信息熵，后者是相对熵；
离散函数：
$H(p,q)=-\sum_x p(x)log q(x)=Entropy(P) + D_KL(P||Q)$

2、在 ML 中等效于相对熵
【作用：用来评估，当前训练得到的概率分布，与真实分布有多么大的差异】
因为与相对熵只差一个 分布 P 的信息熵，
若 P 是固定的分布，与训练无关；
Q 是估计的分布，应尽量等于 P。
二者一致时，交叉熵就等于 P 的熵

二、相对熵
【本质含义：由于编码方案不一定完美，导致的平均编码长度的增大值】
离散：
$D_{KL}(P||Q) = \sum_iP(i)log {P(i)\over Q(i)}$

【发现：$D_KL(P||Q) = ∑P(i) *logP(i) - ∑P(i) *logQ(i)
= - Entropy(P) + 交叉熵 H(p,q)$ 】


1）用来衡量2个取值为正数的函数的相似性
2）2个完全相同的函数，相对熵为0；差异越大，相对熵越大；
3）概率分布函数，或 概率密度函数，若函数值均大于0，相对熵可以度量两个随机分布的差异性；
4）相对熵不对称，没有交换律

# 卷积神经网络 CNN
http://www.36dsj.com/archives/24006
## 简介
一般地，CNN的基本结构包括两层，

- 其一为特征提取层，每个神经元的输入与前一层的局部接受域相连，并提取该局部的特征。一旦该局部特征被提取后，它与其它特征间的位置关系也随之确定下来；
- 其二是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射是一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数。卷积神经网络中的每一个卷积层都紧跟着一个用来求局部平均与二次提取的计算层，这种特有的两次特征提取结构减小了特征分辨率。

    在图像处理中，往往把图像表示为像素的向量，比如一个1000×1000的图像，可以表示为一个1000000的向量。在上一节中提到的神经网络中，如果隐含层数目与输入层一样，即也是1000000时，那么输入层到隐含层的参数数据为1000000×1000000=10^12，这样就太多了，基本没法训练。所以图像处理要想练成神经网络大法，必先减少参数加快速度。就跟辟邪剑谱似的，普通人练得很挫，一旦自宫后内力变强剑法变快，就变的很牛了。


## 局部感知

卷积神经网络有两种神器可以降低参数数目，第一种神器叫做局部感知野。一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。网络部分连通的思想，也是受启发于生物学里面的视觉系统结构。视觉皮层的神经元就是局部接受信息的（即这些神经元只响应某些特定区域的刺激）。如下图所示：左图为全连接，右图为局部连接。
![此处输入图片的描述][1]


  在上右图中，假如每个神经元只和10×10个像素值相连，那么权值数据为1000000×100个参数，减少为原来的千分之一。而那10×10个像素值对应的10×10个参数，其实就相当于卷积操作。

  关于卷积:https://www.zhihu.com/question/22298352

## 参数共享

但其实这样的话参数仍然过多，那么就启动第二级神器，即权值共享。在上面的局部连接中，每个神经元都对应100个参数，一共1000000个神经元，如果这1000000个神经元的100个参数都是相等的，那么参数数目就变为100了。

怎么理解权值共享呢？我们可以这100个参数（也就是卷积操作）看成是提取特征的方式，该方式与位置无关。这其中隐含的原理则是：图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。

更直观一些，当从一个大尺寸图像中随机选取一小块，比如说 8×8 作为样本，并且从这个小块样本中学习到了一些特征，这时我们可以把从这个 8×8 样本中学习到的特征作为探测器，应用到这个图像的任意地方中去。特别是，我们可以用从 8×8 样本中所学习到的特征跟原本的大尺寸图像作卷积，从而对这个大尺寸图像上的任一位置获得一个不同特征的激活值。

如下图所示，展示了一个33的卷积核在55的图像上做卷积的过程。每个卷积都是一种特征提取方式，就像一个筛子，将图像中符合条件（激活值越大越符合条件）的部分筛选出来。
![此处输入图片的描述][2]

## 多卷积核
上面所述只有100个参数时，表明只有1个100*100的卷积核，显然，特征提取是不充分的，我们可以添加多个卷积核，比如32个卷积核，可以学习32种特征。在有多个卷积核时，如下图所示：


  [1]: http://www.36dsj.com/wp-content/uploads/2015/03/511-600x224.jpg
  [2]: http://www.36dsj.com/wp-content/uploads/2015/03/6.gif
