---
title: Analysis
date: 2016-10-20 22:05:06
tags: [DataMining]
categories: [数据挖掘]
---

__定义__:对于一组事务,根据一些事务的项目,发现能预测另外一些事务项目的规则

__Itemset(项集)__:包含一个或者多个条目(item)的集合

__Support count(支持度计数)__:项集出现的频率(次数)

__Support(支持度)__:事务中包含项集的概率.

__Frequent Itemset(频繁项集)__:支持度大于等于最小支持度阈值的项集

__Association Rule(关联规则)__:表示对X->Y这种形式的蕴含.

__Rule Evaluation Metrics(规则评估指标)__

- 支持度: 事务中包含X,Y的概率
- 置信度:衡量Y出现在包含X的事务中的频率

# 关联规则挖掘测试

目标:给定一系列事务T,关联规则挖掘的任务就是去找出所有满足以下条件的规则
- 支持度>=minsup threshold
- 置信度>=minconf threshold

暴力方法:(计算复杂度高)
-列出所有可能的关联规则
-计算每一个规则的支持度和置信度
-修剪去除不符合minsup 和 minconf thresholds的规则


# 关联规则挖掘算法

两个步骤:
1.频繁项集的生成:生成所有支持度>=最小阀值的所有项集
2.规则生成:从每一个频繁项集中生成高置信度规则

> 频繁项集生成的计算成本依旧很大

## 暴力方法
- 在格子内的每一个项集都可能是频繁项集
- 扫描数据库,计算每一个候选项集的支持度

### 计算复杂度
如果有d个独立的items
- 总共有2^d个项集
- 关联规则数为R=(3^d-2^(d+1)+1)

### 频繁项集生成策略

●Reduce the number of candidates (M)
– Complete search: M=2 d
– Use pruning techniques to reduce M
● Reduce the number of transactions (N)
– Reduce size of N as the size of itemset increases
– Used by DHP and vertical-based mining algorithms
● Reduce the number of comparisons (NM)
– Use efficient data structures to store the candidates or transactions
– No need to match every candidate against every transaction

## 减少候选项的数量

- Apriori principle:
  如果一个项集是频繁的,那么它的所有子项集都是频繁的
  一个项集的支持度不大于它的子项集

### Apriori Algorithm
  - 另k=1
  - 生成长度为k的频繁项集
  - 重复直至没有新的频繁项集生成
        - 从长度为k的项集中生成k+1项集
        - 将所有包含长度为k的非频繁子项集的项集剪枝
        - 去处所有非频繁项集,只留下频繁项集

### 减少比较次数
扫描数据库,计算每个候选项集的支持度
将候选项集存储于哈希结构中,减少比较次数

- 生成哈希树(每一个节点最多存储一定数量的项集,超过则节点分裂)
- Subset Operation

### 影响复杂度的元素

1. 最小支持度的选择
2. 数据集的维度
3. 数据集的大小
4. 事务的平均宽度

### FP-Growth Algorithm

FP-tree 建立
1. 扫描数据库,生成1-频繁项集
2. 按降序排列频繁项集
3. 再次扫描频繁项集,生成FP-tree

FP-growth
● Frequent itemsets containing E
● Frequent itemsets containing D but no E
● Frequent itemsets containing C but no D, E
● Frequent itemsets containing B but no C, D, E
● Frequent itemsets containing A but no B, C, D, E

### ECLAT Algorithm

Vertical Data Layout

● Determine support of any k-itemset by intersecting tid-lists of two of its (k-1) subsets.

● 3 traversal approaches:
– top-down, bottom-up and hybrid
● Advantage: very fast support counting
● Disadvantage: intermediate tid-lists may become too
large for memory

## Compact Representation of Frequent Itemsets(频繁项集的紧凑表示)
###Maximal Itemsets & Closed Itemsets
最大频繁项集:　　它的超集都不是频繁项集
闭合项集:　　　　它的超集不存在和他一样的支持度
频繁项集> 闭合项集>最大频繁项集

## MaxMiner Algorithm

> 组成: 头h(n);尾t(n)

- 算法:
- 开始,令节点N,h(N)=0,t(N)={ABCD}
- 展开N:
    - 如果i是t(N)中的,h(N)+i不是频繁的,则展开前将i从t(N)中移除
    - 如果h(N)+t(N)是频繁的,不要展开N

##CLOSET Algorithm
● Creating F_list: <c:4,e:4,f:4,a:3,d:2>
● Divide　search space
Patterns containing d
Patterns containing a but not d
Patterns containing f but not a, d
...

## Revisiting Supports

### Multiple Minimum Support
MS(i): minimum support for item i
Challenge: Apriori principle does not hold.
Suppose:
        Support(Milk, Coke) = 1.5% and
        Support(Milk, Coke, Broccoli) = 0.5%
        {Milk,Coke} is infrequent but {Milk,Coke,Broccoli} is frequent

Order the items according to their minimum
support (in ascending order)
– e.g.:
MS(Milk)=5%,
MS(Coke) = 3%,
MS(Broccoli)=0.1%,
MS(Salmon)=0.5%
– Ordering: Broccoli, Salmon, Coke, Milk

● Need to
modify Apriori such that:
– L 1 : set of frequent items
– F 1 : set of items whose support is 3 MS(1)
where MS(1) is min i ( MS(i) )
– C 2 : candidate itemsets of size 2 is generated from F 1
instead of L 1


Modifications to Apriori:
– In traditional Apriori,
A candidate (k+1)-itemset is generated by merging two
frequent itemsets of size k
The candidate is pruned if it contains any infrequent subsets
of size k
– Pruning step has to be modified:

Prune only if subset contains the first item
e.g.: Candidate={Broccoli, Coke, Milk} (ordered according to
minimum support)
{Broccoli, Coke} and {Broccoli, Milk} are frequent but
{Coke, Milk} is infrequent
– Candidate is not pruned because {Coke,Milk} does not contain
the first item, i.e., Broccoli.
#Rule Generation

给定一个频繁项集Ｌ，找出所有非空子集ｆ属于Ｌ，使得ｆ－＞Ｌ－f,且满足最小置信度要求

If　|L| = k, then there are 2＾k – 2 candidate
association rules (ignoring L－＞空集 and 空集－＞L)

##　Rule Generation for Apriori Algorithm

候选集规则通过将两个在规则结果有相同前缀的规则合并在一块
比如ＣＤ＝＞ＡＢ，ＢＤ＝＞ＡＣ
能够生成Ｄ＝＞ＡＢＣ
如果ＡＤ＝＞ＢＣ的置信度不高，可以将Ｄ＝＞ＡＢＣ减去


# Pattern Evaluation（模型评估）

## Computing Interestingness Measure

Given a rule X ® Y, information needed to compute rule
interestingness can be obtained from a contingency table

## Properties of A Good Measure
