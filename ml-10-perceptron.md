---
title: 统计学习方法-perceptron
date: 2017-03-13 19:59:14
tags: [Maching Learning]
categories: [机器学习]
---
匆忙的一个学期过去了,觉得好像时刻都没懈怠过. 最重要的证明就是自从安上ubuntu而且装windows有冲突时候,唯一方法就是重新格式磁盘再装.
然后我却一个多学期没装windows,也没打游戏.
研一得谢谢好多人,一路有君相伴,甚好.
因为基础知识不扎实,重新看统计学习方法.而且做些小笔记.

# 基础知识
主要的统计方法: 有监督,无监督,半监督,强化学习
基本概念:输入空间,特征空间,输出空间(输入具体为一个实例,通常由特征向量表示,特征向量存在的空间称为特征空间),联合概率分布(变量X与Y之间的联合概率分布),假设空间(输入到输出空间的映射集合)
统计学习三要素:模型(参数存在的空间为参数空间)+策略(损失函数和风险函数)+算法
## 损失函数和风险函数
__损失函数__:代价函数,用来度量模型错误的程度.
常见的损失函数:
1. 0-1损失函数
2. 平方损失函数
3. 绝对值损失函数
4. 对数损失函数 L(Y,P(Y|X)) = -log P(Y|X)   <=> max P(Y|X)
__风险函数(期望损失)__:模型f(X)关于联合概率分布平均意义下的损失.
学习的目标是选择期望风险最小的模型,但是需要用到联合分布,联合分布又是未知的,所以是病态的(ill-formexd)
__经验风险__:数据集的平均损失
由大数定律得知,当样本容量趋于无穷,经验风险趋于期望损失.但是现实中样本容量有限,监督学习中的两个基本策略:__经验风险最小化和结构风险最小化__
__结构风险__:样本容量很小时,会产生过拟合现象.为了防止过拟合而提出来的策略,结构风险最小化等价于正则化,经验风险再加上模型复杂度的正则项(一般为L2风险).
由剃刀原则知道,“切勿浪费较多东西去做用较少的东西同样可以做好的事情。”后来以一种更为广泛的形式为人们所知，即“如无必要，勿增实体。”
## 模型评估和模型选择
__训练误差和测试误差__,误差率和准确率,
__测试误差__ 反应了一个模型对于未知测试数据的预测能力,决定了模型的泛化能力
__正则化__:正则化项可以使模型参数向量的范数.
__交叉验证__:1.简单交叉验证 2.s折交叉验证 3.留一交叉验证(数据缺乏的情况下)

## 泛化能力
学习到的模型,用这个模型对未知数据进行预测的误差称为泛化误差.

## 生成模型和判别模型
生成方法: 由数据学习联合概率分布P(X,Y),然后求出条件概率分布P(Y|X)作为预测的模型,即生成模型,模型表示了给定输入X产生Y的生成关系.
判别方法:给定数据直接学习决策函数f(X)或者条件概率分布f(Y|X).
生成方法的特点:可以还原出联合概率P(X,Y),收敛速度快,可以存在隐变量.
判别方法的特点:可以直接学习条件概率P(Y|X),准确率高,可以定义特征并使用.

分类问题,标注问题,回归问题,

# 感知机
线性模型,判别模型
sign(x) = +1,x>=0 else -1
定义超平面: wx + b = 0
当wx +b > 0,划分为正类,其他划分为负类.
策略: 定义损失函数,损失函数为误分类的点的总距离
经过几何求证,可以知道 误分类的点到超平面S的距离是:  -1/|w| y_i(w*x_i+b)
用梯度下降求解损失函数最小化,其中涉及学习率
算法的收敛性,误分类的次数k是有上界的,有限次搜索能将训练集完全正确分开.
__对偶问题__:参数通过实例x和标记y的线性组合求解其系数求得w和b.先将w和b初始化为0,用x和y表示w,有学习率.看书33
当实例点更新次数越多,意味着它离分类超平面越近,越难正确分类.
